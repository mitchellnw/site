<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>mitchell w | Discovering Neural Wirings</title>
  <meta name="description" content="Website">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/blog/2019/dnw/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>mitchell</strong> w
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
            <a class="page-link" href="/teaching/">teaching</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Discovering Neural Wirings</h1>
    <p class="post-meta">September 18, 2019</p>
  </header>

  <article class="post-content">
    <p>blog by <em>Mitchell Wortsman</em>, <em>Alvaro Herrasti</em>, <em>Sarah Pratt</em>, <em>Ali Farhadi</em> and <em>Mohammad Rastegari</em> from <em>The Allen Institute for Artificial Intelligence</em>, <em>University of Washington</em>, and <em>XNOR.AI</em>.</p>

<p>In this post we discuss the most interesting contributions from our recent paper on <a href="https://arxiv.org/abs/1906.00586"><strong>Discovering Neural Wirings</strong></a>
(NeurIPS 2019). Traditionally, the connectivity patterns of Artificial Neural Networks (ANNs)
are manually defined or largely constrained. In contrast, we relax the typical notion of layers to
allow for a much larger space of possible wirings. The wiring of our ANN is not fixed during
training – as we learn the network parameters we also learn the connectivity.</p>

<p>In our pursuit
we arrive at the following <a href="#sparse-networks-lottery-tickets-overparameterization">conclusion</a>:
it is possible to train a model that is small during inference but still overparameterized during training.
By applying our method to discover sparse ANNs we bridge the gap between
<a href="https://arxiv.org/abs/1611.01578">neural architecture search</a> and <a href="https://arxiv.org/pdf/1907.04840v2.pdf">sparse neural network learning</a>.</p>

<p>Move the slider below to see how the wiring changes when a small network is trained on MNIST for a few epochs (viz code <a href="https://github.com/allenai/neural-wire-viz">here</a>).</p>

<style>
    .slidecontainer {
        width: 100%;
    }

    .slider {
        -webkit-appearance: none;
        width: 100%;
        height: 25px;
        /*background: #d3d3d35e;*/
        outline: none;
        opacity: 0.7;
        -webkit-transition: .2s;
        transition: opacity .2s;
    }

    .slider:hover {
        opacity: 1;
    }

    .slider::-webkit-slider-thumb {
        -webkit-appearance: none;
        appearance: none;
        width: 25px;
        height: 25px;
        background: #4CAF50;
        cursor: pointer;
    }

    .slider::-moz-range-thumb {
        width: 25px;
        height: 25px;
        background: #4CAF50;
        cursor: pointer;
    }

    body {
        background-color: white;
        /*text-align: center;*/
        font-family: sans-serif;
    }

    .slider-text {
        text-align: center;
    }



    input[type=range] {
        -webkit-appearance: none;
        width: 100%;
        margin: 8.35px 0;
    }
    input[type=range]:focus {
        outline: none;
    }
    input[type=range]::-webkit-slider-runnable-track {
        width: 100%;
        height: 10.3px;
        cursor: pointer;
        box-shadow: 1px 1px 1px #000000, 0px 0px 1px #0d0d0d;
        background: rgba(93, 115, 148, 0.34);
        border-radius: 3px;
        border: 0.9px solid #020101;
    }
    input[type=range]::-webkit-slider-thumb {
        box-shadow: 0.7px 0.7px 1.1px #000000, 0px 0px 0.7px #0d0d0d;
        border: 1.9px solid #3d718e;
        height: 27px;
        width: 35px;
        border-radius: 3px;
        background: #ffffff;
        cursor: pointer;
        -webkit-appearance: none;
        margin-top: -9.25px;
    }
    input[type=range]:focus::-webkit-slider-runnable-track {
        background: rgba(105, 128, 161, 0.34);
    }
    input[type=range]::-moz-range-track {
        width: 100%;
        height: 10.3px;
        cursor: pointer;
        box-shadow: 1px 1px 1px #000000, 0px 0px 1px #0d0d0d;
        background: rgba(93, 115, 148, 0.34);
        border-radius: 3px;
        border: 0.9px solid #020101;
    }
    input[type=range]::-moz-range-thumb {
        box-shadow: 0.7px 0.7px 1.1px #000000, 0px 0px 0.7px #0d0d0d;
        border: 1.9px solid #3d718e;
        height: 27px;
        width: 35px;
        border-radius: 3px;
        background: #ffffff;
        cursor: pointer;
    }
    input[type=range]::-ms-track {
        width: 100%;
        height: 10.3px;
        cursor: pointer;
        background: transparent;
        border-color: transparent;
        color: transparent;
    }
    input[type=range]::-ms-fill-lower {
        background: rgba(83, 103, 132, 0.34);
        border: 0.9px solid #020101;
        border-radius: 6px;
        box-shadow: 1px 1px 1px #000000, 0px 0px 1px #0d0d0d;
    }
    input[type=range]::-ms-fill-upper {
        background: rgba(93, 115, 148, 0.34);
        border: 0.9px solid #020101;
        border-radius: 6px;
        box-shadow: 1px 1px 1px #000000, 0px 0px 1px #0d0d0d;
    }
    input[type=range]::-ms-thumb {
        box-shadow: 0.7px 0.7px 1.1px #000000, 0px 0px 0.7px #0d0d0d;
        border: 1.9px solid #3d718e;
        height: 27px;
        width: 35px;
        border-radius: 3px;
        background: #ffffff;
        cursor: pointer;
        height: 10.3px;
    }
    input[type=range]:focus::-ms-fill-lower {
        background: rgba(93, 115, 148, 0.34);
    }
    input[type=range]:focus::-ms-fill-upper {
        background: rgba(105, 128, 161, 0.34);
    }

    .slider-container {
        display: flex;
        justify-content: center;
    }

    .slider {
        width: 40%;
        height: 100%;
    }

    #error {
        position: absolute;
        top: 50%;
        left: 30%;
    }


</style>

<script src="https://d3js.org/d3.v4.min.js"></script>

<script src="/js/demo/d3-save-svg.js"></script>

<script src="/js/demo/mnist-slider.js"></script>

<div>
    <svg width="100%" height="55vmin" style="min-height: 300px;" id="d3-svg" onload="start('https://dnw-demo-api.s3-us-west-2.amazonaws.com')"></svg>
    <div class="contols">
        <div id="error"></div>
        <div class="slider-container">
            <div class="slider">
                <input id="epoch-slider" value="0" type="range" min="0" max="100" />
            </div>
        </div>
        <div class="output-container">
            <p id="slider-output" class="slider-text">0</p>
        </div>
    </div>
</div>

<!--<script src="https://dnw-demo-api.s3-us-west-2.amazonaws.com/d3-save-svg.min.js"></script>-->
<!--<script src="https://dnw-demo-api.s3-us-west-2.amazonaws.com/mnist-slider.js"></script>-->

<h3 id="why-wiring">Why Wiring?</h3>

<p>Before the advent of modern ANNs, researchers would manually engineer good features (high dimensional vector representations). Good features may now be learned with ANNs, but the architecture of the ANN must be specified instead. Accordingly, a myriad of recent work in <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search (NAS)</a> has focused on learning the architecture of an ANN.
However, NAS still searches among a set of manually designed building blocks and so ANN connectivity remains largely constrained. In contrast, <a href="https://arxiv.org/pdf/1904.01569.pdf">RandWire</a> explores a diverse set of connectivity patterns by considering ANNs which are randomly wired.
Although randomly wired ANNs are competitive with NAS, their connectivity is fixed during training.</p>

<p>We propose a method for jointly learning the parameters and wiring of an ANN during training. We demonstrate that our method of <em>Discovering Neural Wirings (DNW)</em> outperforms many manually-designed and randomly wired ANNs.</p>

<p>ANNs are inspired by the biological neural networks of the animal brain. Despite the countless fundamental differences between these two systems, a biological inspiration may still prove useful. A recent Nature Communications article (aptly titled <a href="https://www.nature.com/articles/s41467-019-11786-6">A critique of pure learning and what artificial neural networks can learn from animal brains</a>) argues that the connectivity of an animal brain enables rapid learning. Accordingly, the article suggests <em>“wiring topology and network architecture as a target for optimization in artificial systems.”</em> We hope that this work provides a useful step in this direction.</p>

<p>Concurrent work on <a href="https://weightagnostic.github.io/">Weight Agnostic Neural Networks</a> also emphasizes the importance of ANN wiring. They demonstrate that a given wiring for an ANN can effectively solve some simple tasks without any training – the solution is encoded in the connectivity.</p>

<h3 id="static-neural-graphs-sngs-a-convenient-abstraction-for-a-feed-forward-ann"><em>Static Neural Graphs (SNGs)</em>: A Convenient Abstraction for a Feed Forward ANN</h3>

<p>We now describe a convenient abstraction for a feed-forward ANN – a <em>Static Neural Graph (SNG)</em>. Our goal is then to learn the optimal <em>edge set</em> of the SNG.
We skim over some low level details below and invite you to reference the <a href="https://arxiv.org/abs/1906.00586">paper</a>,
though this abstraction should feel familiar.</p>

<p>An <em>SNG</em> is a directed acyclic graph <script type="math/tex">G</script> which consists of nodes <script type="math/tex">\mathcal{V}</script> and edges <script type="math/tex">\mathcal{E}</script>. Additionally, each node <script type="math/tex">v</script> has output <script type="math/tex">\mathcal{Z}_v</script> and input <script type="math/tex">\mathcal{I}_v</script>. Input data <script type="math/tex">X</script> flows into the network through a designated set of nodes <script type="math/tex">\mathcal{V}_0</script>, and the input to node <script type="math/tex">v \in \mathcal{V} \setminus \mathcal{V}_0</script> is a weighted sum of the parent’s outputs</p>

<script type="math/tex; mode=display">\mathcal{I}_v = \sum_{(u,v) \in \mathcal{E}} w_{uv}\mathcal{Z}_u</script>

<p>The output of each node is computed via a parameterized function</p>

<script type="math/tex; mode=display">\mathcal{Z}_v = f_{\theta_v}\left(\mathcal{I}_v\right)</script>

<p>where the edge weights <script type="math/tex">w_{uv}</script> and <script type="math/tex">\theta_v</script> are learnable network parameters. The output of the network is then computed via a designated set of nodes <script type="math/tex">\mathcal{V}_E</script>.</p>

<p>In this work we are designing models for Computer Vision, and so each node resembles a single <em>channel</em> (as illustrated below).
Accordingly, <script type="math/tex">f_{\theta_v}</script> performs a convolution over a 2-dimensional matrix (followed by <a href="https://arxiv.org/abs/1502.03167">BatchNorm</a> and ReLU).</p>

<div style="text-align:center"><img src="/assets/img/dnw_fig.png" width="200" /></div>

<h3 id="an-algorithm-for-discovering-neural-wirings">An Algorithm for Discovering Neural Wirings</h3>

<p>How do we learn the optimal edge set <script type="math/tex">\mathcal{E}</script> during training? We follow recent work (such as <a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis</a>) and attribute the “importance” of a parameter to its magnitude. Accordingly, the link between node <script type="math/tex">u</script> and <script type="math/tex">v</script> is considered <em>important</em> if <script type="math/tex">\lvert w_{uv} \rvert \gg 0</script>.</p>

<p>At each training iteration the edge set is chosen by taking the highest magnitude weights:</p>

<script type="math/tex; mode=display">\mathcal{E} = \{ (u,v) \ : \ \lvert w_{uv} \rvert > \tau \textrm{ and } u \lt v \}</script>

<p>where <script type="math/tex">\tau</script> is chosen so that there are exactly <script type="math/tex">k</script> edges and <script type="math/tex">u \lt v</script> ensures that the graph is acyclic.</p>

<p>All that remains is to choose a weight update for <script type="math/tex">w_{uv}</script>. Recall that most use <a href="https://www.nature.com/articles/323533a0">backpropogation</a>
where gradients from a loss term <script type="math/tex">\mathcal{L}</script> are passed backwards through the network. Using the chain rule gradients may be computed with respect to each network parameter.
The parameters are then often updated via <em>stochastic gradient descent</em> with some learning rate <script type="math/tex">\alpha</script>. Conveniently, standard backprop will automatically compute the quantity</p>

<script type="math/tex; mode=display">g_v = - \alpha \frac{\partial \mathcal{L}}{\partial \mathcal{I}_v}</script>

<p>Informally, the quantity <script type="math/tex">g_v</script> describes how the network <em>wants</em> <script type="math/tex">\mathcal{I}_v</script> to change so that the loss will decrease.
Our rule is therefore to strengthen the connection between <script type="math/tex">u</script> and <script type="math/tex">v</script> when <script type="math/tex">\mathcal{Z}_u</script> aligns with <script type="math/tex">g_v</script>.
In other words, if node <script type="math/tex">u</script> can take <script type="math/tex">\mathcal{I}_v</script> where the loss <em>wants</em> it to go, we should increase the edge weight from <script type="math/tex">u</script> to <script type="math/tex">v</script>. We therefore modify <script type="math/tex">w_{uv}</script> via our update rule</p>

<script type="math/tex; mode=display">w_{uv} \gets w_{uv} + \left\langle \mathcal{Z}_u, g_v \right\rangle</script>

<p>where <script type="math/tex">\langle \cdot, \cdot \rangle</script> denotes an inner product (these quantities are implicitly treated as vectors).</p>

<p>In practice <script type="math/tex">w_{uv}</script> changes only a small amount at each training iteration.
However, if <script type="math/tex">\mathcal{Z}_u</script> consistently aligns with <script type="math/tex">g_v</script> then <script type="math/tex">\lvert w_{uv} \rvert</script> will strengthen to a point where edge <script type="math/tex">(u,v)</script> replaces a weaker edge. We show <a href="#proofs">below</a> that when swapping does occur, it is beneficial under some assumptions.</p>

<p>When training, the rest of the network is updated via backprop as usual. In fact, you may notice that the update rule exactly resembles SGD when <script type="math/tex">(u,v) \in \mathcal{E}</script>. And so the algorithm may be interpreted equivalently as allowing the gradient to flow to, but not through, a set of “potential” edges. In practice we include a momentum and weight decay term as is standard practice with SGD (weight decay should eventually remove dead ends).</p>

<p>As we show in the <a href="https://arxiv.org/abs/1906.00586">paper</a>, this is equivelantly a <a href="https://arxiv.org/abs/1308.3432">straight-through estimator</a>.</p>

<h3 id="putting-it-together">Putting it Together</h3>
<p><img src="/assets/gif/dnw.gif" width="600" /></p>

<h3 id="wirings-at-scale">Wirings at Scale</h3>

<p>We employ the following two strategies for discovering wirings at scale:</p>

<ol>
  <li>
    <p>We chain together multiple graphs, <script type="math/tex">G_1, G_2, ..., G_n</script> where the output of <script type="math/tex">G_i</script> is the input to <script type="math/tex">G_{i+1}</script>. The <em>input nodes</em> perform a strided convolution and the spatial resolution is fixed throughout the remaining nodes in the graph.</p>
  </li>
  <li>
    <p>The depth of the graph is limited to be <script type="math/tex">\ell</script> by partitioning the nodes into blocks <script type="math/tex">\mathcal{B}_1,...,\mathcal{B}_\ell</script>. We then only allow connections between nodes <script type="math/tex">u \in \mathcal{B}_i</script> and <script type="math/tex">v \in \mathcal{B}_j</script> if <script type="math/tex">i \lt j</script>.</p>
  </li>
</ol>

<p>For an even comparison, we consider the exact same structure and number of edges as MobileNet V1 if it were interpreted as a chain of graphs. By learning the connectivity we boost the ImageNet accuracy by ~10% in the low compute setting.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Model</th>
      <th style="text-align: center">ImageNet Top-1 Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Original MobileNet V1 (x 0.25)</td>
      <td style="text-align: center">50.6 %</td>
    </tr>
    <tr>
      <td style="text-align: center">Random Graph MobileNet V1 (x 0.225)</td>
      <td style="text-align: center">53.3 %</td>
    </tr>
    <tr>
      <td style="text-align: center">Discovered MobileNet V1 (x 0.225)</td>
      <td style="text-align: center">60.9 %</td>
    </tr>
  </tbody>
</table>

<h3 id="sparse-networks-lottery-tickets-overparameterization">Sparse Networks? Lottery Tickets? Overparameterization?</h3>

<p>The past few years have witnessed illuminating work in the field of sparse ANNs.
In <a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis</a>,
Frankle and Carbin demonstrate that dense ANNs contain
subnetworks that can be effectively trained in isolation. However,
their process for uncovering these so-called <em>winning tickets</em> is
expensive as it first requires a dense network to be trained.
In <a href="https://arxiv.org/pdf/1907.04840.pdf">Sparse Networks from Scratch</a>,
Dettmers and Zettlemoyer introduce <em>sparse learning</em> – training ANNs only once while maintaining sparse weights throughout.</p>

<p>Our work aims to unify the problem of neural architecture search
with sparse neural network learning. As NAS becomes less restrictive and more fine grained,
finding a good architecture is akin to finding a sparse subnetwork of the complete graph.</p>

<p>Accordingly, we may use our algorithm for Discovering Neural Wirings and apply it to
the task of training other sparse ANNs. Our method requires no fine-tuning
or retraining to discover a sparse subnetwork. This perspective was guided
by Dettmers and Zettelmoyer, though we would like to highlight some differences.
Their work enables faster training, though our
backwards pass is still dense. Moreover, their work allows a
redistribution of parameters across layers whereas we consider a fixed sparisty per layer.
Finally, their
training is more memory efficient as they actually send unused
weights to zero while we continue to update them in the backwards pass.</p>

<p>We leave the biases and batchnorm dense and use
a <a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5">ResNet-50 v1.5</a>.
This mirrors the experimental set-up from Appendix C of <a href="https://arxiv.org/pdf/1907.04840v2.pdf">Sparse Networks from Scratch</a>.
The figure below illustrates how top-1 accuracy varies with the sparsity
(of the convolutional filters and linear weight – a sparsity of 0% corresponds to
the dense network). The figure also shows an alternative setting where the first convolutional
layer (with &lt; 10k parameters <script type="math/tex">\approx</script> 0.04% of the total network) is left dense.</p>

<style>
    .accuracy-plot-container {
      font: 10px sans-serif;
      font-weight: bold;
    }

    .axis path,
    .axis line {
      fill: none;
      stroke: #000;
      shape-rendering: crispEdges;
    }

    .x.axis path {
      display: none;
    }

    .line {
      fill: none;
      stroke: steelblue;
      stroke-width: 1.5px;
    }

</style>

<div id="accuracy-plot" class="accuracy-plot-container"></div>

<script>


function getSvgDimensions(svgId) {
    let svgEl = document.getElementById(svgId);
    let rect = svgEl.getBoundingClientRect();
    return rect;
}

var myData = "Sparsity	All Layers Sparse	First Layer Dense\n\
0\t77.510\t77.510\t\n\
10\t77.488\t77.382\t\n\
20\t77.610\t77.284\t\n\
30\t77.604\t77.492\t\n\
40\t77.536\t77.564\t\n\
50\t77.528\t77.570\t\n\
60\t77.290\t77.456\t\n\
70\t76.904\t77.076\t\n\
80\t76.158\t76.600\t\n\
90\t74.026\t75.044\t\n";


var margin = {
  top:20,
  right:100,
  bottom:50,
  left:40
};



var line = d3.line()
.x(function(d){
  return scaleX(d.Sparsity)
})
.y(function(d){
  return scaleY(d.temperature)
})
.curve(d3.curveBasis);

var svg = d3.select("#accuracy-plot").append("svg")
  .attr('id', 'plot-svg')
.attr("width", '100%')
.attr("height", '50vmin')
// .style("background-color","lightGreen")
.append("g")
.attr("transform","translate("+(margin.left) +","+margin.top+")")

let rect = getSvgDimensions('plot-svg');
console.log("SVG rect ", rect);


var w = rect.width - margin.left - margin.right;
var h = rect.height - margin.top - margin.bottom;

var parseDate = (x) => parseInt(x)

var scaleX = d3.scaleLinear()
  .range([0,w]);

var scaleY = d3.scaleLinear()
  .range([h,0]);

var color = d3.scaleOrdinal(d3.schemeCategory10);


var xAxis = d3.axisBottom()
  .scale(scaleX)
  .tickFormat(function(d, i) {
    return i === 0 ? 'Sparsity' : d;
  });

var yAxis = d3.axisLeft()
  .scale(scaleY)


var data = d3.tsvParse(myData);
console.log("data is",data)

color.domain(d3.keys(data[0]).filter(function(key){
  console.log("key",key)
  return key!=="Sparsity";

}))


data.forEach(function(d){
  d.Sparsity = parseDate(d.Sparsity);

});

var cities = color.domain().map(function(name){
  return {
    name:name,
    values:data.map(function(d){
      return {
        Sparsity:+d.Sparsity,
        temperature:+d[name]
      };
    })
  };
});

scaleX.domain(d3.extent(data,function(d){
  return d.Sparsity;
}));
scaleY.domain([d3.min(cities,function(c){
  return d3.min(c.values,function(v){
    return v.temperature
  })
}),d3.max(cities,function(c){
  return d3.max(c.values,function(v){
    return v.temperature;
  })
})])

console.log("cities",cities);

var legend = svg.selectAll("g")
.data(cities)
.enter()
.append("g")
.attr("class","legend");

legend.append("rect")
.attr("x",w-80)
.attr("y",function(d,i){
  return i * 20;
})
.attr("width",10)
.attr("height",10)
.style("fill",function(d){
  return color(d.name);
});

legend.append("text")
.attr("x",w-62)
.attr("y",function(d,i){
  return (i * 20) + 9;
})
.text(function(d){
  return d.name;
});

svg.append("g")
.attr("class","x axis")
.attr("transform","translate(0,"+h+")")
.call(xAxis);

svg.append("g")
.attr("class","y axis")
.call(yAxis)
.append("text")
.attr("transform","rotate(-90)")
.attr("y",6)
  .attr("x",-7)
.attr("dy",".71em")
.style("text-anchor","end")
.style("fill","black")
.text("ImageNet Top-1 Accuracy");

var city = svg.selectAll(".city")
.data(cities)
.enter().append("g")
.attr("class","city");

city.append("path")
.attr("class","line")
.attr("d",function(d){
  return line(d.values);
})
.style("stroke",function(d){
  return color(d.name)
});

city.append("text")
.datum(function(d){
  // let lastSpace = d.name.lastIndexOf(' ');
  //
  // let name = d.name.length > 17?  d.name.substring(0, lastSpace)+"\n"+ d.name.substring(lastSpace + 1, d.name.length): d.name;
  // // let name = d.name.length > 17 ? 'www' : d.name;
  return{
    name:d.name,
    value:d.values[d.values.length -1]
  };
})
.attr("transform",function(d){
  return "translate(" + scaleX(d.value.Sparsity)+","+scaleY(d.value.temperature)+")";
})
.attr("x",3)
.attr("dy",".35")
.text(function(d){
  return d.name;
});

var mouseG = svg.append("g") // this the black vertical line to folow mouse
.attr("class","mouse-over-effects");

mouseG.append("path")
.attr("class","mouse-line")
.style("stroke","black")
.style("stroke-width","1px")
.style("opacity","0");

var lines = document.getElementsByClassName("line");
var mousePerLine = mouseG.selectAll(".mouse-per-line")
.data(cities)
.enter()
.append("g")
.attr("class","mouse-per-line");

mousePerLine.append("circle")
.attr("r",7)
.style("stroke",function(d){
  return color(d.name);
})
.style("fill", "none")
.style("stroke-width", "1px")
.style("opacity", "0");

mousePerLine.append("text")
.attr("transform","translate(10,3)");

mouseG.append("rect")
.attr("width",w)
.attr("height",h)
.attr("fill","none")
.attr("pointer-events","all")
.on("mouseout",function(){
  d3.select(".mouse-line").style("opacity","0");
  d3.selectAll(".mouse-per-line circle").style("opacity","0");
  d3.selectAll(".mouse-per-line text").style("opacity","0")
})
.on("mouseover",function(){
  d3.select(".mouse-line").style("opacity","1");
  d3.selectAll(".mouse-per-line circle").style("opacity","1");
  d3.selectAll(".mouse-per-line text").style("opacity","1")

})
.on("mousemove",function(){

  var mouse = d3.mouse(this);

  d3.select(".mouse-line")
  .attr("d",function(){
    var d = "M" + mouse[0] +"," + h;
    d+=" " +mouse[0] + "," + 0;
    return d;
  })

  d3.selectAll(".mouse-per-line")
  .attr("transform",function(d,i){

    var xDate = scaleX.invert(mouse[0]),
    bisect =d3.bisector(function(d){ return d.Sparsity;}).right;
    idx = bisect(d.values,xDate);




    var beginning = 0,
     end = lines[i].getTotalLength(),
    target = null;



    while(true){
      target = Math.floor((beginning+end)/2)

      pos = lines[i].getPointAtLength(target);


      if((target ===end || target == beginning) && pos.x !==mouse[0]){
        break;
      }

      if(pos.x > mouse[0]) end = target;
      else if(pos.x < mouse[0]) beginning = target;
      else break; // position found
    }
    d3.select(this).select("text")
    .text(scaleY.invert(pos.y).toFixed(1))
    .attr("fill",function(d){
      return color(d.name)
    });
    return "translate(" +mouse[0]+","+pos.y+")";

  });



});

// original graph: https://bl.ocks.org/larsenmtl/e3b8b7c2ca4787f77d78f58d41c3da91

</script>

<p>To generate the figure above we consider only multiples of 10% and the rest is
interpolated. All models and numbers will may be found at our <a href="https://github.com/allenai/dnw">Github</a>
though we provide relevant <strong>ImageNet Top-1 Accuracy</strong> metrics for ResNet-50 v1.5 below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Model</th>
      <th style="text-align: center">10% of Weights</th>
      <th style="text-align: center">20% of Weights</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">First Layer Dense (ours)</td>
      <td style="text-align: center">75.0 %</td>
      <td style="text-align: center">76.6 %</td>
    </tr>
    <tr>
      <td style="text-align: center">All Layers Sparse (ours)</td>
      <td style="text-align: center">74.0 %</td>
      <td style="text-align: center">76.2 %</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/pdf/1907.04840v2.pdf">Sparse Networks from Scratch (Dettmers &amp; Zettelmoyer)</a></td>
      <td style="text-align: center">72.9 %</td>
      <td style="text-align: center">74.9 %</td>
    </tr>
  </tbody>
</table>

<p>We would like to highlight an interesting conclusion we may draw from this result:
<strong>It is possible to realize the benefits of overparameterization during training even when the resulting model is sparse.</strong>
Though we only ever use a small percentage
of the weights during the forwards pass, our network has good odds at winning the <a href="https://arxiv.org/pdf/1907.04840v2.pdf">initialization lottery</a>.</p>

<p>The implementation for training sparse ANNs with our algorithm is quite simple. We implicitly treat
each parameter as an edge and so
all convolutions are replaced with the following <a href="https://pytorch.org/">pytorch</a> code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">autograd</span>


<span class="k">class</span> <span class="nc">ChooseEdges</span><span class="p">(</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">prune_rate</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="nb">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
        <span class="n">p</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">prune_rate</span> <span class="o">*</span> <span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
        <span class="c1"># flat_oup and output access the same memory.
</span>        <span class="n">flat_oup</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">flat_oup</span><span class="p">[</span><span class="n">idx</span><span class="p">[:</span><span class="n">p</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">grad_output</span><span class="p">,</span> <span class="bp">None</span>


<span class="k">class</span> <span class="nc">SparseConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">set_prune_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prune_rate</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prune_rate</span> <span class="o">=</span> <span class="n">prune_rate</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">ChooseEdges</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prune_rate</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>The figure below illustrates how this code works. It also emphasizes that we are using a <a href="https://arxiv.org/abs/1308.3432">straight-through estimator</a> (see paper for more discussion).</p>

<div style="text-align:center"><img src="/assets/img/dnw_code_fig.png" width="600" /></div>

<h3 id="discovering-neural-wirings-for-dynamic-neural-graphs-dngs">Discovering Neural Wirings for <em>Dynamic Neural Graphs (DNGs)</em></h3>

<p>We may also apply our algorithm to processes on graphs, where nodes receive
input and produce output at all times <script type="math/tex">t</script> (and the graph is not restricted to be a DAG).
In the <em>discrete time</em> setting, we consider times
<script type="math/tex">t \in \{0, 1,..., T\}</script> and let the input and output vary with <script type="math/tex">t</script>
(we skim over some low level details – i.e. the initial conditions – and invite
you to reference the <a href="https://arxiv.org/abs/1906.00586">paper</a>). The state of
node <script type="math/tex">v</script> at time <script type="math/tex">t \gt 0</script> is then given by</p>

<script type="math/tex; mode=display">\mathcal{Z}_v(t) = f_{\theta_v}\left(\sum_{(u,v) \in \mathcal{E}} w_{uv}Z_u(t-1)\right)</script>

<p>We may also consider the setting where <script type="math/tex">t</script> takes on a <em>continuous</em> range of values (as in <a href="https://arxiv.org/abs/1806.07366">Neural Ordinary Differential Equations</a>). The ANN evolves according to the following dynamics:</p>

<script type="math/tex; mode=display">{d \mathcal{Z}_v(t) \over d t} = f_{\theta_v}\left(\sum_{(u,v) \in \mathcal{E}} w_{uv}Z_u(t)\right)</script>

<p>We apply our algorithm for Discovering Neural Wirings to a tiny (41k parameter)
classifier in both the <em>static</em> and <em>dynamic</em> setting.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Model</th>
      <th style="text-align: center">Accuracy (CIFAR-10)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Static (Random Graph)</td>
      <td style="text-align: center">76.1 <script type="math/tex">\pm</script> 0.5</td>
    </tr>
    <tr>
      <td style="text-align: center">Static (ours)</td>
      <td style="text-align: center">80.9 <script type="math/tex">\pm</script> 0.6</td>
    </tr>
    <tr>
      <td style="text-align: center">Discrete Time (Random Graph)</td>
      <td style="text-align: center">77.3<script type="math/tex">\pm</script> 0.7</td>
    </tr>
    <tr>
      <td style="text-align: center">Discrete Time (ours)</td>
      <td style="text-align: center">82.3<script type="math/tex">\pm</script> 0.6</td>
    </tr>
    <tr>
      <td style="text-align: center">Continuous (Random Graph)</td>
      <td style="text-align: center">78.5 <script type="math/tex">\pm</script> 1.2</td>
    </tr>
    <tr>
      <td style="text-align: center">Continuous (ours)</td>
      <td style="text-align: center">83.1 <script type="math/tex">\pm</script> 0.3</td>
    </tr>
  </tbody>
</table>

<div style="text-align:center"><img src="/assets/img/dnw_dynamic_fig.png" width="250" /></div>

<h3 id="proofs">Proofs</h3>

<p>Here we briefly show that when swapping edges does occur, it is beneficial under some assumptions.
Consider a <em>Static Neural Graph</em> where edge <script type="math/tex">(i,k)</script> replaces edge <script type="math/tex">(j,k)</script> after the gradient update
for the mini-batch. We may show that when the learning rate <script type="math/tex">\alpha</script> is sufficiently small and the node states are fixed then the loss will decrease for the current mini-batch. We skim over some details here
and invite you to reference the <a href="https://arxiv.org/abs/1906.00586">paper</a>, for example we must also
assume that the loss is Lipschitz continuous. The <a href="https://arxiv.org/abs/1906.00586">paper</a> also
includes a proof of a more general case.</p>

<p>We let <script type="math/tex">\tilde w</script> denote the weight <script type="math/tex">w</script> after the gradient update. And so by our update rule we let</p>

<p><script type="math/tex">\tilde w_{uv} = w_{uv} + \left\langle \mathcal{Z}_u, g_v \right\rangle</script> where <script type="math/tex">g_v = -\alpha {\partial \mathcal{L} \over \partial \mathcal{I}_v}</script>. If the learning rate is sufficiently small then
<script type="math/tex">\tilde w</script> will be close to <script type="math/tex">w</script> and so we assume that <script type="math/tex">\tilde w w \gt 0</script>. Since edge <script type="math/tex">(i,k)</script> replaces edge <script type="math/tex">(j,k)</script> we know that <script type="math/tex">\lvert w_{ik} \rvert \lt \lvert w_{jk} \rvert</script> and <script type="math/tex">\lvert \tilde w_{ik} \rvert \gt \lvert \tilde w_{jk} \rvert</script> as we choose edges by taking the highest magnitude weights.</p>

<p>Let <script type="math/tex">\mathcal{A}</script> be the new input to node <script type="math/tex">v</script> if swapping <em>is</em> allowed. Likewise, let <script type="math/tex">\mathcal{B}</script> be the new input to node <script type="math/tex">v</script> if swapping <em>is not</em> allowed. It suffices
to show that <script type="math/tex">\langle \mathcal{A} - \mathcal{I_k}, g_k \rangle \gt \langle \mathcal{B} - \mathcal{I_k}, g_k \rangle</script> as we observe that
<script type="math/tex">\mathcal{L}(\mathcal{\mathcal{I}_k}) - \mathcal{L}(\mathcal{A}) =  \langle \mathcal{A} - \mathcal{I_k}, g_k \rangle + \mathcal{O}(\alpha^2)</script> via a Taylor approximation.</p>

<p><script type="math/tex">\mathcal{A}</script> may be written as <script type="math/tex">\tilde w_{ik} \mathcal{Z}_i + \sum_{(u,k) \in \mathcal{E} , \ u\neq i,j} w_{uk} \mathcal{Z}_u</script> and <script type="math/tex">\mathcal{B}</script> may be written as <script type="math/tex">\tilde w_{jk} \mathcal{Z}_j + \sum_{(u,k) \in \mathcal{E}, \ u\neq i,j} w_{uk} \mathcal{Z}_u</script> and so</p>

<p><script type="math/tex">\langle \mathcal{A} - \mathcal{I_k}, g_k \rangle \gt \langle \mathcal{B} - \mathcal{I_k}, g_k \rangle \iff \tilde w_{ik} \left\langle \mathcal{Z}_i, g_k \right\rangle > \tilde w_{jk} \left\langle \mathcal{Z}_j, g_k \right\rangle</script>.</p>

<p>If the magnitude of <script type="math/tex">w_{ik}</script> increases while the magnitude of <script type="math/tex">w_{jk}</script> decreases then we are done as the left hand side will be positive while the right hand side will be negative. We now examine the case where both magnitudes either increase or increase.</p>

<p>As <script type="math/tex">\tilde w_{ik} - w_{ik} = \langle \mathcal{Z}_i, g_k \rangle</script> we are left to show that <script type="math/tex">\tilde w_{ik}^2 - w_{ik}\tilde w_{ik} > \tilde w_{jk}^2 - w_{jk}\tilde w_{jk}</script>. Moreover, since  <script type="math/tex">\tilde w w \gt 0</script> we may simplify to <script type="math/tex">\lvert \tilde w_{ik}\rvert^2 - \lvert w_{ik} \rvert \lvert \tilde w_{ik} \rvert > \lvert \tilde w_{jk} \rvert ^2 - \lvert w_{jk} \rvert \lvert \tilde w_{jk} \rvert</script>.</p>

<p>Rearranging we find that
<script type="math/tex">\lvert w_{jk} \rvert \lvert \tilde w_{jk}\rvert  - \lvert w_{ik} \rvert \lvert \tilde w_{ik} \rvert > \lvert \tilde w_{jk} \rvert ^2 -  \lvert \tilde w_{ik}\rvert^2</script> where the right hand side is negative by assumption. Moreover, when the learning rate is small enough the left hand side is positive. If <script type="math/tex">\epsilon = \lvert w_{jk} \rvert - \lvert w_{ik} \rvert</script>. In the extreme case where <script type="math/tex">\lvert \tilde w_{ik} \rvert</script> <em>just exceeds</em> <script type="math/tex">\lvert \tilde w_{jk} \rvert</script> then</p>

<p><script type="math/tex">\lvert w_{jk} \rvert \lvert \tilde w_{jk}\rvert  - \lvert w_{ik} \rvert \lvert \tilde w_{ik} \rvert \approx \epsilon  \lvert \tilde w_{jk} \rvert > 0</script>.</p>

<h3 id="citing">Citing</h3>

<p>If you found this work to be helpful please consider citing:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Wortsman2019DiscoveringNW,
  title={Discovering Neural Wirings},
  author={Mitchell Wortsman and Ali Farhadi and Mohammad Rastegari},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.00586}
}
</code></pre></div></div>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We sincerely thank Tim Dettmers for his assistance and guidance in the experiments regarding sparse networks.</p>

<h3 id="comments-and-faq">Comments and FAQ</h3>

<p>To comment raise an issue on our github repo <a href="https://github.com/allenai/dnw">here</a>.</p>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2022 mitchell w.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
