<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>mitchell w | Supermasks in Superposition</title>
  <meta name="description" content="Website">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/blog/2020/supsup/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>mitchell</strong> w
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
            <a class="page-link" href="/teaching/">teaching</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Supermasks in Superposition</h1>
    <p class="post-meta">June 23, 2020</p>
  </header>

  <article class="post-content">
    <p><a href="https://arxiv.org/abs/2006.14769">Supermasks in Superposition</a> is a project by
<em>Mitchell Wortsman</em><script type="math/tex">^*</script>,
<em><a href="https://vkramanuj.github.io/">Vivek Ramanujan</a></em><script type="math/tex">^*</script>,
<em><a href="http://rosanneliu.com/">Rosanne Liu</a></em>,
<em><a href="https://anikem.github.io/">Ani Kembhavi</a></em>,
<em><a href="https://scholar.google.com/citations?user=N4-2Z_cAAAAJ&amp;hl=en">Mohammad Rastegari</a></em>,
<em><a href="http://yosinski.com/">Jason Yosinski</a></em>,
and <em><a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a></em>.</p>

<p>Learning many different tasks sequentially remains a challenge for Artificial Neural Networks (ANNs).
When learning a new task, networks tend to <a href="https://arxiv.org/abs/1612.00796"><em>catastrophically forget</em></a>
information from previous tasks. In this post we discuss Supermasks in Superposition (SupSup),
a flexible model capable of learning thousands of tasks without forgetting (and without access to task identity info).</p>

<p>SupSup leverages the expressive power of neural network connectivity. <a href="https://eng.uber.com/deconstructing-lottery-tickets/">Zhou <em>et al.</em></a>
demonstrates that even when the weights of an ANN are random and fixed, learning can still occur by modifying the network connectivity.
Instead of learning the weights, or <a href="https://mitchellnw.github.io/blog/2019/dnw/">jointly learning the weights and connectivity</a>,
good performance can be achieved by choosing whether each edge in the the network is <em>on</em> or <em>off</em>. In our <a href="https://arxiv.org/pdf/1911.13299.pdf">previous work</a> (<a href="https://www.youtube.com/watch?v=C6Tj8anJO-Q">video</a>) we provide a fast algorithm
for finding these so-called <strong>supermasks</strong> (binary masks which specify if each edge is on or off, exposing a subnetwork with good performance).</p>

<p>SupSup uses a neural network with weights that remain fixed and random. For each new task <script type="math/tex">i</script> SupSup finds a supermask (subnetwork)
which achieves good performance on task <script type="math/tex">i</script>. The underlying weights are not modified and so forgetting does not occur. Moreover,
the underlying weights have minimal storage cost as you only need to save the random seed.</p>

<div style="text-align:center"><img src="/assets/img/supsup_training.png" width="500" /></div>

<p>When the task identity of data is given during inference, the corresponding supermask can be used
(similar to <a href="https://arxiv.org/abs/1801.06519">Mallya <em>et al.</em></a> but without the need for a pretrained backbone).</p>

<p>SupSup may also be used even when task identity is unknown during inference (<em>i.e.</em> the model does not have access to which
task it is currently evaluating on). SupSup does so by first inferring task identity and then using the corresponding supermask (subnetwork).</p>

<p>We consider classification tasks, and so the network outputs form a probability distribution over the possible classes.
The work of <a href="https://arxiv.org/abs/1610.02136">Hendrycks &amp; Gimpel</a> suggests that the supermask trained on task <script type="math/tex">i</script> is more likely to be confident when given data from task <script type="math/tex">i</script>.
Therefore, SupSup infers task identity by finding the supermask which produces the lowest entropy (highest confidence) output.</p>

<div style="text-align:center"><img src="/assets/img/supsup_inference.png" width="500" /></div>

<p>As the number of tasks grows large (&gt; 1000), it becomes impractical to iterate through each supermask and check the entropy
of the output distribution. Therefore, we consider a network with <em>all</em> supermasks in weighted
<strong>superposition</strong>—mask <script type="math/tex">i</script> has coefficient <script type="math/tex">\alpha_i</script> (where each <script type="math/tex">\alpha_i > 0</script> and <script type="math/tex">\sum_i \alpha_i = 1</script>). We can then infer task identity
using gradient based optimization to minimize the entropy of the ouputs by modifying the coefficients <script type="math/tex">\alpha_i</script>.
In practice we find that a single gradient computation can be sufficient, even for 2500 tasks. Moreover, the speed
of the forward and backward pass (<em>i.e.</em> computing outputs and the entropy gradient) with the superimposed model is
faster than sequentially trying each supermask learned so far.</p>

<p>SupSup performs well in a variety of different settings for continual learning, where different tasks are learned
sequentially. For clarity we characterize the continual learning scenarios with a useful taxonomy and mnemonic.</p>

<h3 id="continual-learning-scenarios">Continual Learning Scenarios</h3>
<p>In our <a href="https://arxiv.org/abs/2006.14769">paper</a> we provide a more complete characterization and justification for different continual learning scenarios.
However, the key differences between scenarios are whether <strong>1)</strong> the model has access to which task it is performing inference or training on
and <strong>2)</strong> whether tasks reuse labels. Consider, for example, the task of learning 50 permuations of MNIST. Is the model evaluated on correctly identifying the digit
or is the model required to identify the digit and permutation? The table below provides our taxonomy, which builds upon
<a href="https://arxiv.org/abs/1904.07734">van de Ven &amp; Tolias</a> and <a href="https://arxiv.org/abs/1803.10123">Zeno <em>et al.</em></a>.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Scenario</th>
      <th style="text-align: center">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">GG</td>
      <td style="text-align: center">Task <strong>G</strong>iven during train and <strong>G</strong>iven during inference</td>
    </tr>
    <tr>
      <td style="text-align: center">GNs</td>
      <td style="text-align: center">Task <strong>G</strong>iven during train, <strong>N</strong>ot inference; <strong>s</strong>hared labels</td>
    </tr>
    <tr>
      <td style="text-align: center">GNu</td>
      <td style="text-align: center">Task <strong>G</strong>iven during train, <strong>N</strong>ot inference; <strong>u</strong>nshared labels</td>
    </tr>
    <tr>
      <td style="text-align: center">NNs</td>
      <td style="text-align: center">Task <strong>N</strong>ot given during train <strong>N</strong>or inference; <strong>s</strong>hared labels</td>
    </tr>
  </tbody>
</table>

<h3 id="gg-task-given-during-train-given-during-inference">GG: Task <strong>G</strong>iven during train, <strong>G</strong>iven during inference</h3>

<p>We consider fixed, random weights <script type="math/tex">W</script> and for each task <script type="math/tex">i</script> we learn a binary mask <script type="math/tex">M^i</script> such that the network
with outputs <script type="math/tex">\mathbf{p} = f(\mathbf{x}, M^i \odot W)</script> achieves good performance. Note that <script type="math/tex">\mathbf{x}</script> denotes
the input data and <script type="math/tex">\mathbf{p}</script> denotes the output class probabilities. We can control the storage cost of SupSup by
changing the sparsity of the supermasks. Below we provide performance on the challenging
task of <a href="https://arxiv.org/abs/2002.06715">SplitImageNet</a>, where <a href="http://www.image-net.org/">ImageNet</a> is split into 100 different 10-way classification
problems learned sequentially. For Upper Bound, a new set of weights is trained for each task and bytes is storage cost.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Algorithm</th>
      <th style="text-align: center">Avg Top 1 Accuracy (SplitImageNet)</th>
      <th style="text-align: center">Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Upper Bound</td>
      <td style="text-align: center">92.55</td>
      <td style="text-align: center">10222.81M</td>
    </tr>
    <tr>
      <td style="text-align: center">SupSup (less sparse)</td>
      <td style="text-align: center">89.58</td>
      <td style="text-align: center">195.18M</td>
    </tr>
    <tr>
      <td style="text-align: center">SupSup (more sparse)</td>
      <td style="text-align: center">86.37</td>
      <td style="text-align: center">65.50M</td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://arxiv.org/abs/2002.06715">BatchE</a></td>
      <td style="text-align: center">81.50</td>
      <td style="text-align: center">124.99M</td>
    </tr>
  </tbody>
</table>

<h3 id="gn-task-given-during-train-not-inference">GN: Task <strong>G</strong>iven during train, <strong>N</strong>ot inference</h3>

<p>As mentioned above, SupSup can be used to infer the task identity of given data.
We consider a network with <em>all</em> supermasks in weighted
superposition—mask <script type="math/tex">i</script> has coefficient <script type="math/tex">\alpha_i</script>
(initially each <script type="math/tex">\alpha_i = 1/k</script> where <script type="math/tex">k</script> is the number of tasks learned so far).
Outputs with the superimposed model are then given by</p>

<script type="math/tex; mode=display">\mathbf{p}_\text{sup} = f\left(\mathbf{x}, \left( \sum_i \alpha_i M^i \right) \odot W\right)</script>

<p>where <script type="math/tex">\odot</script> denotes an elementwise product. SupSup infers task identity by minimizing entropy <script type="math/tex">\mathcal{H}</script> with gradient
based optimization, as the correct suprmask should produce a low entropy output. In this blog post we only provide results
for the most inexpensive setting, where <script type="math/tex">\textbf{x}</script> is a single data point and task identity is inferred with a
single gradient computation via</p>

<script type="math/tex; mode=display">\text{argmax}_i  \left(- \frac{\partial \mathcal{H}\left(\mathbf{p}_\text{sup}\right)}{\partial \alpha_i}\right)</script>

<p>as entropy is decreasing maximally in this coordinate.</p>

<p>We show results below for LeNet-300-100 (left) and a fully
connected network with two hidden layers of size 1024 (right) trained sequentially on 250 permutations of
the MNIST pixels (1000 training batches per task). In this scenario SupSup outperforms methods such as <a href="https://arxiv.org/abs/2002.06715">BatchE</a> and <a href="https://papers.nips.cc/paper/9269-superposition-of-many-models-into-one">PSP</a>
which operate in scenario GG—they have access to the task during inference. Results are provided for
both entropy <script type="math/tex">\mathcal{H}</script> and an alternative metric <script type="math/tex">\mathcal{G}</script> described in the <a href="https://arxiv.org/abs/2006.14769">paper</a>.</p>

<div style="text-align:center"><img src="/assets/img/supsup_v1v2.png" width="650" /></div>

<h3 id="nn-task-not-given-during-train-nor-inference">NN: Task <strong>N</strong>ot given during train <strong>N</strong>or inference</h3>

<p>SupSup can also be extended to the scenario where task information is not available, even during training—the model does
not know when tasks switch. We consider PermutedMNIST, where each task is a different permutation of the MNIST pixels.
We train on each task for 1000 batches (the model does not have access to this iteration number).
Every 100 batches the model must choose to allocate a new mask or pick an existing mask. SupSup allocates a new mask
if it is uncertain when inferring task identity. If SupSup is uncertain about task identity
(<script type="math/tex">\nabla_\alpha \mathcal{H}</script> is approximately uniform) then the data
likely doesn’t belong to a task learned so far.</p>

<p>We illustrate below that SupSup is able to sequentially learn 2500 permutations of MNIST with minimal forgetting.</p>

<div style="text-align:center"><img src="/assets/img/supsup_long.png" width="650" /></div>

<h3 id="and-more">And more…</h3>

<p>See the <a href="https://arxiv.org/abs/2006.14769">paper</a> for more thorough experiments and methods, which include storing the
entire, growing set of supermasks in a constant-sized reservoir by
implicitly encoding them as attractors in a fixed-sized <a href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield network</a>.</p>

<h3 id="comments-and-faq">Comments and FAQ</h3>

<p>To comment raise an issue on our <a href="https://github.com/RAIVNLab/supsup">github repo</a>.
We will post and answer FAQ here.</p>

  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2022 mitchell w.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
